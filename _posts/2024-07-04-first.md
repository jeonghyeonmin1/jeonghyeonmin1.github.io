---
layout: posts
title: "1주차"
---

# 논문 리뷰

---

## 계획
Challenge1) SAR segmentation (7.18 발표)

Goal : (precision:90 recall:80 || F1score:87) &&  연산량 GFlops=0.5  (FLOPS = 속도(GPU), FLOPs = 양(모델))
- 연산량 낮을수록 좋은거

논문 리뷰 1/2~3days(2Q per 1 + 아이디어 도출)

오마카세 : 1/2week 화(7pm) - start(7.2)

논문리뷰 : 매주 월/수(2pm)(발표날에는 생략) + 이때 질문하면서 논문 쓸 주제 생각해보기(~7.31)

---

## 결과

### \<MobileNetV1\>
너비,깊이 나눠서 연산

- depth-wise convolution (공간 방향으로 연산)

![사진](/assets/image/2024-07-04-first-0.png){: width="60%" height="60%" .align-center}

- Point-wise Convolution(공간 X, 채널 방향으로 연산)

![사진](/assets/image/2024-07-04-first-1.png){: width="60%" height="60%" .align-center}

- depth-wise Convolution + point-wise Convolution

![사진](/assets/image/2024-07-04-first-2.png){: width="60%" height="60%" .align-center}


### \<mobileNetV2\>
병목현상 :  정보가 몰릴 때 중요한 정보만 통과할거다
<br><br>

128 →  32 : 중요한 정보로만 압축 → 128 : 확장시켜 중요한 정보들로만 채움  —>  불필요한 정보를 없애서 성능 향상에 도움됨.
<br><br>

inverted bottleneck

—> 기존 bottleneck 은 Relu로 인해 중요한 정보가 절반으로 줄어듦

그래서 반대로 확장시켜 Relu를 사용해도 중요한 정보 손실이 나지 않고 non-linearity를 유지할 수 있게함

128 —> 512 —> 256 —> 128(128~256) 이런 식으로 기존에 병목현상과 다르게 정보 손실이 줄어듦

근데, 이렇게 하면 연산량이 많아지는데 —> mobileNetV1 에서 사용된 depth-wise convolution 을 통해 연산량이 그렇게 많아지지 않게 함. —> 그래서 확장시켜도 되는거임


### \<MobileNetV3\>

NAS : 딥러닝을 통해 네트워크 아키텍처를 찾아주는 기술

swish : 음수 값이여도 갱생의 기회를 줌

![사진](/assets/image/2024-07-04-first-3.png){: width="60%" height="60%" .align-center}

![사진](/assets/image/2024-07-04-first-4.png){: width="60%" height="60%" .align-center}

hswish: swish 에서 e^x 계산이 복잡해져서 보완하기 위해 나옴

![사진](/assets/image/2024-07-04-first-5.png){: width="60%" height="60%" .align-center}

MobileNetV3 정리 ⇒ NAS 를 통해 모델 구조를 뽑아내고 hard-swish를 적용시켰더니 성능이 향상되더라.
