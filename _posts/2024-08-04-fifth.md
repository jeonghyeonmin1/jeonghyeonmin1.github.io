---
layout: posts
title: "5주차"
---

# 논문 리뷰

---

## 계획
1. Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs 

2. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
--- 

## 결과

### <Scaling Up Your Kernels to 31*31>

### 31*31 크기의 large kernel 등장 배경
: Vit 에서 영감을 받아 큰 커널을 사용하는 CNN 구조를 제안하게 됨 -> 이 방법은 작은 커널을 여러개 사용하는 대신
몇 개의 큰 커널을 사용하는 것이 더 효율적임을 보여줌

![사진](/assets/image/2024-08-04-first-0.png){: width="60%" height="60%" .align-center}


### large kernel 사용으로 인한 이점

1. 큰 커널을 사용한 CNN이 작은 커널을 사용할 때보다 유효 수용 영역(ERF)가 증가함
-> 유효 수용 영역(ERF) 이론에 따르면, ERF는 커널 크기(K)와 깊이(L)에 비례하여 증가하는데
깊이가 증가하면 최적화가 어려워지는 문제점이 생김
-> 근데 large kernel을 사용하면 레이어를 적게 쌓아도 큰 ERF를 얻어 깊이 증가로 인한 최적화 문제도 해결 가능.

<br>

2. large kernel을 사용한 CNN은 높은 shape bias 를 가짐 
-> Vit(비전 트랜스포머)는 인간의 시각 시스템과 유사하게 형태를 기반으로 예측을 내리는데, 
기존의 CNN은 주로 local texture에 초점을 맞추는 경향이 강함
-> 그런데 large kernel을 이용해 기존의 CNN 에서도 Vit를 사용할 때와 비슷한 shape bias를 가질 수 있음을 보임 

<br>

* shape-bias 관련 이점 정리 : 기존에는 모델이 texture(질감)를 더 많이 봐서 예측에 잘못하는 경우(적대적 공격)가 생겼음
-> 그래서 형태를 위주로 예측하려는 시도들이 많아짐 -> 이렇게해서 Vit가 등장해 이로인해 기존의 CNN을 이용할때보다 
shpae bias가 높아져 인간의 시각과 비슷하게 판단을 내릴 수 있게 됨 -> 근데 여기서 기존의 CNN에 large kernel을 도입하니
Vit처럼 CNN에서도 비슷한 shape bias 를 얻을 수 있게 됨

<br>

### \<CutMix\>

### 등장 배경 
: 기존의 Regional dropout은 informative한 픽셀의 정보를 까맣게 처리하거나 무작위 노이즈로 처리함
-> 이로인해 정보손실 + 비효율성을 초래해 바람직하지 않다고 받아들여짐 -> CutMix 라는 증강 기법을 제안

<br>

### CutMix 개념
: 한 이미지의 패치를 잘라내어 훈련 이미지에 붙이고 라벨도 똑같이 섞어줌 
-> 입력 데이터의 일부를 무작위로 제거하는 Regional dropout과 유사한 정규화 효과를 제공해 과적합을 방지 
+ 잘못된 입력에 대한 Model Robustness를 향상시켜줌 -> 모델이 훈련된 데이터와 다른 데이터를 감지하는 성능을 개선시켜줌

<br>

![사진](/assets/image/2024-08-04-second-0.png){: width="60%" height="60%" .align-center}

![사진](/assets/image/2024-08-04-second-1.png){: width="60%" height="60%" .align-center}
